{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeGyJI-jaXWj",
        "outputId": "22be5c6f-93ed-4079-87fd-155001d752f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "authenticated\n"
          ]
        }
      ],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "print('authenticated')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import pickle\n",
        "import gzip\n",
        "import json\n",
        "import sys"
      ],
      "metadata": {
        "id": "l7c4cBeldjOp"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to load the dataset\n",
        "download the [mnist dataset](https://drive.google.com/file/d/1l_zwAKQTlZPib4xhVGriDpVqjaXkXZk-/view?usp=drive_link) or used your desired dataset *(donot forget to change the nerons and other parameters)*\n",
        "\n",
        "upload the downloaded gzip file in the colab file\n",
        "\n",
        "\n",
        "> remember: donot forget to check the correct file name at the line 7\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZEhJZkO373rp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load 50,000 dataset from mnist\n",
        "def load_data():\n",
        "  # training data is 28*28 pixel\n",
        "  # return value =: training data, the validation data, and the test data.\n",
        "\n",
        "  print('loading dataset...')\n",
        "  f = gzip.open('/content/mnist.pkl.gz', 'rb') # --> you can replace the file name with your desire file name\n",
        "\n",
        "  u = pickle._Unpickler(f)\n",
        "  u.encoding = 'latin1'\n",
        "  training_data, validation_data, test_data = u.load()\n",
        "  f.close()\n",
        "  print('loaded')\n",
        "  return (training_data, validation_data, test_data)\n",
        "\n",
        "def load_data_wrapper():\n",
        "  tr_d, va_d, te_d = load_data()\n",
        "  print('reshapping...')\n",
        "  training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
        "  training_results = [vectorized_result(y) for y in tr_d[1]]\n",
        "  training_data = list(zip(training_inputs, training_results))\n",
        "\n",
        "  validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
        "  validation_data = list(zip(validation_inputs, va_d[1]))\n",
        "\n",
        "  test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
        "  test_data = list(zip(test_inputs, te_d[1]))\n",
        "  print('completed')\n",
        "  return (training_data, validation_data, test_data)\n",
        "\n",
        "def vectorized_result(j):\n",
        "  \"\"\"\n",
        "  This is used to convert a digit\n",
        "  (0...9) into a corresponding desired output\n",
        "  \"\"\"\n",
        "  e = np.zeros((10, 1))\n",
        "  e[j] = 1.0\n",
        "  return e"
      ],
      "metadata": {
        "id": "eKEI2y0VR7Zw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helping Function"
      ],
      "metadata": {
        "id": "WB42Wqc87y1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(z):\n",
        "  exp_z = np.exp(z - np.max(z))\n",
        "  return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
        "\n",
        "def sigmoid(z):\n",
        "  return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "  # derivative of sigmoid fn\n",
        "  return sigmoid(z)*(1-sigmoid(z))\n",
        "\n",
        "def load(filename):\n",
        "  # load instance of the Network from a file\n",
        "  f = open(filename, 'r')\n",
        "  data = json.load(f)\n",
        "  f.close()\n",
        "\n",
        "  cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
        "  net = Network(data[\"sizes\"], cost=cost)\n",
        "  net.weights = [np.array(w) for w in data[\"weights\"]]\n",
        "  net.biases = [np.array(b) for b in data[\"biases\"]]\n",
        "\n",
        "  return net\n"
      ],
      "metadata": {
        "id": "_ERgL3hFc-Un"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cost Caluation method\n",
        "\n",
        "**Cross-entropy** is better than **Quadractic** for training because we have eliminated the direct multiplication of the sigmoid derivative in the gradient calculation. This helps avoid the issue of vanishing gradients when the network's predictions are very wrong, leading to faster and more stable training.\n",
        "\n",
        "*When the network is way off, it learns quicker, that would be the case from the very start*\n",
        "\n",
        "**NOTE** :In Cross Entropy Cost, we used np.nan_to_num is used to ensure numerical stability.  In particular, if both `a` and `y` have a 1.0\n",
        "in the same slot, then the expression (1-y)*np.log(1-a)\n",
        "returns nan.  The np.nan_to_num ensures that that is converted\n",
        "to the correct value (0.0)."
      ],
      "metadata": {
        "id": "t_0vjecY3ppH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  class QuadraticCost(object):\n",
        "    @staticmethod\n",
        "    def fn(a, y):\n",
        "      return 0.5*np.linalg.norm(a-y)**2\n",
        "\n",
        "    @staticmethod\n",
        "    def delta(z, a, y):\n",
        "      return (a-y)*sigmoid_prime(z)\n",
        "\n",
        "  class CrossEntropyCost(object):\n",
        "    @staticmethod\n",
        "    def fn(a, y):\n",
        "      return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
        "\n",
        "    @staticmethod\n",
        "    def delta(z, a, y):\n",
        "      \"\"\"\n",
        "      we eliminated sigmoid derivative function from the error delta\n",
        "      which allow this neural network to learn even faster when its more wrong\n",
        "      \"\"\"\n",
        "      return (a-y)"
      ],
      "metadata": {
        "id": "xu9VqBDq2UWX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network Class and their necessary methods"
      ],
      "metadata": {
        "id": "ZjTMaAcR9Vr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(object):\n",
        "  def __init__(self, sizes, cost=CrossEntropyCost):\n",
        "    self.num_layers = len(sizes)\n",
        "    self.sizes = sizes\n",
        "    # self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "    # self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\\\n",
        "    self.default_weight_init()\n",
        "    self.cost = cost\n",
        "\n",
        "  def default_weight_init(self):\n",
        "    \"\"\"Initialize each weight using a Gaussian distribution with mean 0\n",
        "        and standard deviation 1 over the square root of the number of\n",
        "        weights connecting to the same neuron\"\"\"\n",
        "    self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
        "    self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
        "\n",
        "  def large_weight_init(self):\n",
        "    \"\"\"Initialize the weights using a Gaussian distribution with mean 0\n",
        "        and standard deviation 1\n",
        "    \"\"\"\n",
        "    self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
        "    self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
        "\n",
        "  def feedforward(self, a):\n",
        "    for w, b in zip(self.weights, self.biases):\n",
        "      a = sigmoid(np.dot(w, a) + b)\n",
        "    return a\n",
        "\n",
        "  # def cost_derivative(self, output_activations, y):\n",
        "  #   return (output_activations - y)\n",
        "\n",
        "  def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
        "          lmbda = 0.0,\n",
        "            test_data=None,\n",
        "            monitor_evaluation_cost=False,\n",
        "            monitor_evaluation_accuracy=False,\n",
        "            monitor_training_cost=False,\n",
        "            monitor_training_accuracy=False):\n",
        "\n",
        "    print(\"training...\")\n",
        "    if test_data: n_test = len(test_data)\n",
        "    n = len(training_data)\n",
        "    evaluation_cost, evaluation_accuracy = [], []\n",
        "    training_cost, training_accuracy = [], []\n",
        "\n",
        "    for j in range(epochs):\n",
        "      time1 = time.time()\n",
        "      random.shuffle(training_data)\n",
        "      mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
        "\n",
        "      for mini_batch in mini_batches:\n",
        "        self.update_mini_batch(mini_batch, eta, lmbda, len(training_data))\n",
        "\n",
        "      print(\"Epoch %s training complete\" % j)\n",
        "\n",
        "      if monitor_training_cost:\n",
        "        cost = self.total_cost(training_data, lmbda)\n",
        "        training_cost.append(cost)\n",
        "        print(\"Cost on training data: {}\".format(cost))\n",
        "      if monitor_training_accuracy:\n",
        "        accuracy = self.accuracy(training_data, convert=True)\n",
        "        training_accuracy.append(accuracy)\n",
        "        print(\"Accuracy on training data: {} / {} = {}\".format(accuracy, n, accuracy/n))\n",
        "      if monitor_evaluation_cost:\n",
        "        cost = self.total_cost(test_data, lmbda, convert=True)\n",
        "        evaluation_cost.append(cost)\n",
        "        print(\"Cost on evaluation data: {}\".format(cost))\n",
        "      if monitor_evaluation_accuracy:\n",
        "        accuracy = self.accuracy(test_data)\n",
        "        evaluation_accuracy.append(accuracy)\n",
        "        print(\"Accuracy on evaluation data: {} / {} = {}\".format(self.accuracy(test_data), n_test, self.accuracy(test_data)/n_test))\n",
        "\n",
        "    return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
        "\n",
        "  def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
        "    \"\"\"Update the network's weights and biases by applying gradient\n",
        "        descent using backpropagation to a single mini batch\"\"\"\n",
        "    nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "    nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "    # print(\"calculating gradient...\")\n",
        "    for x, y in mini_batch:\n",
        "      delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "\n",
        "      nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "      nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "\n",
        "    # print(\"updating weights...\")\n",
        "\n",
        "    self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
        "    # self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
        "    self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "\n",
        "  def backprop(self, x, y):\n",
        "    \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
        "        gradient for the cost function C_x.\"\"\"\n",
        "    nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "    nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "    #feedforward\n",
        "    activation = x\n",
        "    activations = [x] # layer by layer(LBL) activation\n",
        "    zs = [] # z vector store LBL\n",
        "    for b, w in zip(self.biases, self.weights):\n",
        "      z = np.dot(w, activation) + b\n",
        "      activation = sigmoid(z)\n",
        "      activations.append(activation)\n",
        "      zs.append(z)\n",
        "\n",
        "    # backpass\n",
        "    delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
        "    nabla_b[-1] = delta\n",
        "    nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "\n",
        "    for l in range(2, self.num_layers):\n",
        "      delta = np.dot(self.weights[-l+1].transpose(), delta)*sigmoid_prime(zs[-l])\n",
        "      nabla_b[-l] = delta\n",
        "      nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "    return (nabla_b, nabla_w)\n",
        "\n",
        "  def evaluate(self, test_data):\n",
        "    test_results = [(np.argmax(self.feedforward(x)), y) for x,y in test_data]\n",
        "    return sum(int(x == y) for (x, y) in test_results)\n",
        "\n",
        "  def accuracy(self, data, convert=False):\n",
        "    \"\"\"Return the number of inputs in ``data`` for which the neural\n",
        "        network outputs the correct result\"\"\"\n",
        "    if convert:\n",
        "      results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in data]\n",
        "    else:\n",
        "      results = [(np.argmax(self.feedforward(x)), y) for (x, y) in data]\n",
        "    return sum(int(x == y) for (x, y) in results)\n",
        "\n",
        "  def total_cost(self, data, lmbda, convert=False):\n",
        "    \"\"\"The flag\n",
        "        ``convert`` should be set to False if the data set is the\n",
        "        training data (the usual case), and to True if the data set is\n",
        "        the validation or test data.\n",
        "        \"\"\"\n",
        "    cost = 0.0\n",
        "    for x, y in data:\n",
        "      a = self.feedforward(x)\n",
        "      if convert: y = vectorized_result(y)\n",
        "      cost += self.cost.fn(a, y)/len(data)\n",
        "      cost += 0.5*(lmbda/len(data))*sum(np.linalg.norm(w)**2 for w in self.weights)\n",
        "    return cost\n",
        "\n",
        "  def save(self, filename):\n",
        "    data = {\"sizes\": self.sizes,\n",
        "            \"weights\": [w.tolist() for w in self.weights],\n",
        "            \"biases\": [b.tolist() for b in self.biases],\n",
        "            \"cost\": str(self.cost.__name__)}\n",
        "    f = open(filename, \"w\")\n",
        "    json.dump(data, f)\n",
        "    f.close()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-E_HoCoiFd4L"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main function\n",
        "you can find some input boxes and slider to change the parameters"
      ],
      "metadata": {
        "id": "iyjvt0AC9lUz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Tl-r54luhDWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if(__name__ == \"__main__\"):\n",
        "  training_data, validation_data, test_data = load_data_wrapper()\n",
        "\n",
        "  testing_data = test_data # @param [\"test_data\", \"validation_data\"] {type: \"raw\"}\n",
        "\n",
        "  Cost = CrossEntropyCost #@param [\"QuadraticCost\", \"CrossEntropyCost\"] {type:\"raw\"}\n",
        "  Hidden_Layer = \"784 155 10\" # @param {\"type\":\"string\"}\n",
        "  hidden_layer = [int(x) for x in Hidden_Layer.split()]\n",
        "\n",
        "  net = Network(hidden_layer, cost=Cost)\n",
        "  net.default_weight_init()\n",
        "\n",
        "  Epochs = 14 # @param {type: \"number\"}\n",
        "  Mini_Batch_Size = 10 # @param {\"type\":\"slider\",\"min\":0,\"max\":100,\"step\":1}\n",
        "  Learning_Rate = 0.5 # @param {\"type\":\"slider\",\"min\":0,\"max\":1,\"step\":0.01}\n",
        "  Regularization = 0.75 # @param {\"type\":\"slider\",\"min\":0,\"max\":1,\"step\":0.05}\n",
        "\n",
        "  Monitor_Evaluation_Accuracy = True # @param {type:\"boolean\"}\n",
        "  Monitor_Evaluation_Cost = False # @param {type:\"boolean\"}\n",
        "  Monitor_Training_Accuracy = False # @param {type:\"boolean\"}\n",
        "  Monitor_Training_Cost = False # @param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "  net.SGD(training_data=training_data,\n",
        "          epochs=Epochs,\n",
        "          mini_batch_size=Mini_Batch_Size,\n",
        "          eta=Learning_Rate,\n",
        "          lmbda = Regularization,\n",
        "          test_data=testing_data,\n",
        "          monitor_evaluation_accuracy=Monitor_Evaluation_Accuracy,\n",
        "          monitor_evaluation_cost=Monitor_Evaluation_Cost,\n",
        "          monitor_training_accuracy=Monitor_Training_Accuracy,\n",
        "          monitor_training_cost=Monitor_Training_Cost\n",
        "        )\n",
        "\n",
        "  Save = True # @param {type:\"boolean\"}\n",
        "  if Save:\n",
        "    net.save(\"net.json\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipRN_iTawuAC",
        "outputId": "64062d21-de3f-40a7-9d39-cb5ec7ed392c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading dataset...\n",
            "loaded\n",
            "reshapping...\n",
            "completed\n",
            "training...\n",
            "Epoch 0 training complete\n",
            "Accuracy on evaluation data: 9570 / 10000 = 0.957\n",
            "Epoch 1 training complete\n",
            "Accuracy on evaluation data: 9649 / 10000 = 0.9649\n",
            "Epoch 2 training complete\n",
            "Accuracy on evaluation data: 9725 / 10000 = 0.9725\n",
            "Epoch 3 training complete\n",
            "Accuracy on evaluation data: 9741 / 10000 = 0.9741\n",
            "Epoch 4 training complete\n",
            "Accuracy on evaluation data: 9757 / 10000 = 0.9757\n",
            "Epoch 5 training complete\n",
            "Accuracy on evaluation data: 9785 / 10000 = 0.9785\n",
            "Epoch 6 training complete\n",
            "Accuracy on evaluation data: 9796 / 10000 = 0.9796\n",
            "Epoch 7 training complete\n",
            "Accuracy on evaluation data: 9780 / 10000 = 0.978\n",
            "Epoch 8 training complete\n",
            "Accuracy on evaluation data: 9789 / 10000 = 0.9789\n",
            "Epoch 9 training complete\n",
            "Accuracy on evaluation data: 9788 / 10000 = 0.9788\n",
            "Epoch 10 training complete\n",
            "Accuracy on evaluation data: 9797 / 10000 = 0.9797\n",
            "Epoch 11 training complete\n",
            "Accuracy on evaluation data: 9799 / 10000 = 0.9799\n",
            "Epoch 12 training complete\n",
            "Accuracy on evaluation data: 9792 / 10000 = 0.9792\n",
            "Epoch 13 training complete\n",
            "Accuracy on evaluation data: 9794 / 10000 = 0.9794\n"
          ]
        }
      ]
    }
  ]
}